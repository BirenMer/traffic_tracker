{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910cca83-f360-4830-89e6-1a7b2e5d4a54",
   "metadata": {},
   "source": [
    "#### PIPE - 6:    \n",
    "This pipe covers the following : \n",
    "1. All features of pipe 5\n",
    "2. Blurring rest image (mild) while saving it for evidance in evidance folder Keep both images\n",
    "3. Selecting images based on size for enhancement to save time\n",
    "4. Detecting Image using YOLO model two (Part one of Phase one ANRP)\n",
    "5. Cropping liscense plate images for OCR purpose (Part one of Phase one ANRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c534dd-6ad5-4728-95cf-74d6aacc7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-17 13:51:21.320\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"models/osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_detected : 5\n",
      "bbox_list- [484, 237, 581, 319]\n",
      "for 5 speed is -> 32\n",
      "violation_detected : 10\n",
      "bbox_list- [463, 223, 570, 329]\n",
      "for 10 speed is -> 38\n",
      "violation_detected : 28\n",
      "bbox_list- [558, 250, 598, 305]\n",
      "for 28 speed is -> 33\n",
      "violation_detected : 34\n",
      "bbox_list- [476, 256, 500, 301]\n",
      "for 34 speed is -> 37\n",
      "violation_detected : 72\n",
      "bbox_list- [369, 230, 464, 326]\n",
      "for 72 speed is -> 34\n",
      "violation_detected : 88\n",
      "bbox_list- [626, 256, 653, 301]\n",
      "for 88 speed is -> 25\n",
      "violation_detected : 107\n",
      "bbox_list- [356, 222, 452, 329]\n",
      "for 107 speed is -> 38\n",
      "violation_detected : 113\n",
      "bbox_list- [418, 221, 523, 336]\n",
      "for 113 speed is -> 37\n",
      "violation_detected : 174\n",
      "bbox_list- [345, 238, 435, 322]\n",
      "for 174 speed is -> 24\n",
      "for 192 speed is -> 17\n",
      "violation_detected : 169\n",
      "bbox_list- [465, 239, 558, 319]\n",
      "for 169 speed is -> 24\n",
      "violation_detected : 180\n",
      "bbox_list- [461, 237, 545, 316]\n",
      "for 180 speed is -> 33\n",
      "violation_detected : 213\n",
      "bbox_list- [417, 228, 517, 324]\n",
      "for 213 speed is -> 39\n",
      "violation_detected : 214\n",
      "bbox_list- [437, 236, 535, 322]\n",
      "for 214 speed is -> 45\n",
      "violation_detected : 220\n",
      "bbox_list- [543, 252, 574, 299]\n",
      "for 220 speed is -> 27\n",
      "violation_detected : 223\n",
      "bbox_list- [436, 229, 541, 328]\n",
      "for 223 speed is -> 30\n",
      "for 232 speed is -> 10\n",
      "violation_detected : 239\n",
      "bbox_list- [332, 239, 420, 323]\n",
      "for 239 speed is -> 47\n",
      "violation_detected : 241\n",
      "bbox_list- [357, 237, 445, 318]\n",
      "for 241 speed is -> 45\n",
      "violation_detected : 274\n",
      "bbox_list- [501, 239, 600, 320]\n",
      "for 274 speed is -> 41\n",
      "violation_detected : 263\n",
      "bbox_list- [393, 230, 494, 324]\n",
      "for 263 speed is -> 32\n",
      "violation_detected : 315\n",
      "bbox_list- [386, 234, 484, 327]\n",
      "for 315 speed is -> 44\n",
      "violation_detected : 320\n",
      "bbox_list- [493, 224, 609, 329]\n",
      "for 320 speed is -> 31\n",
      "violation_detected : 367\n",
      "bbox_list- [490, 238, 593, 321]\n",
      "for 367 speed is -> 25\n",
      "violation_detected : 401\n",
      "bbox_list- [343, 225, 436, 329]\n",
      "for 401 speed is -> 36\n",
      "violation_detected : 422\n",
      "bbox_list- [481, 235, 576, 319]\n",
      "for 422 speed is -> 25\n",
      "violation_detected : 455\n",
      "bbox_list- [566, 254, 596, 301]\n",
      "for 455 speed is -> 28\n",
      "violation_detected : 522\n",
      "bbox_list- [624, 257, 648, 298]\n",
      "for 522 speed is -> 30\n",
      "violation_detected : 530\n",
      "bbox_list- [391, 224, 488, 328]\n",
      "for 530 speed is -> 42\n",
      "violation_detected : 557\n",
      "bbox_list- [473, 232, 580, 325]\n",
      "for 557 speed is -> 37\n",
      "violation_detected : 637\n",
      "bbox_list- [530, 253, 564, 299]\n",
      "for 637 speed is -> 39\n",
      "violation_detected : 650\n",
      "bbox_list- [563, 253, 595, 298]\n",
      "for 650 speed is -> 25\n",
      "violation_detected : 735\n",
      "bbox_list- [533, 255, 562, 301]\n",
      "for 735 speed is -> 34\n",
      "violation_detected : 742\n",
      "bbox_list- [611, 252, 638, 300]\n",
      "for 742 speed is -> 29\n",
      "for 774 speed is -> 17\n",
      "violation_detected : 794\n",
      "bbox_list- [449, 224, 555, 336]\n",
      "for 794 speed is -> 36\n",
      "for 876 speed is -> 19\n",
      "violation_detected : 852\n",
      "bbox_list- [467, 224, 578, 328]\n",
      "for 852 speed is -> 21\n",
      "total_time 1165.5973296165466\n",
      "CPU times: user 47min 58s, sys: 1h 32min 18s, total: 2h 20min 16s\n",
      "Wall time: 19min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import threading \n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from boxmot import StrongSORT\n",
    "from utils.utils import *\n",
    "start_time=time.time()\n",
    "down = {}\n",
    "up = {}\n",
    "text_color = (0, 0, 0)  # Black color for text\n",
    "yellow_color = (0, 255, 255)  # Yellow color for background\n",
    "red_color = (0, 0, 255)  # Red color for lines\n",
    "blue_color = (255, 0, 0)  # Blue color for lines\n",
    "\n",
    "counter_down = []\n",
    "counter_up = []\n",
    "\n",
    "first_boundry_y=170\n",
    "\n",
    "red_line_y = 198\n",
    "\n",
    "blue_line_y = 280\n",
    "\n",
    "offset = 6\n",
    "# Specify the start and end points of the line\n",
    "start_point = (300, 198)\n",
    "end_point = (300, 280)\n",
    "\n",
    "all_id_tracker=[]\n",
    "voilation_id_tracker=[]\n",
    "violation_frame_tracker={}\n",
    "tracker=tracker_init(cuda_device=torch.cuda.is_available(),cuda_device_number=1)\n",
    "model=YOLO('yolov8s.pt')\n",
    "video_path='/media/hlink/hd/test_videos/testx_vid_1.mp4'\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_main = cv2.VideoWriter('output_strongs.avi', fourcc, 20.0, (1020, 500))\n",
    "frame_count=0\n",
    "all_frames_record_path=create_directory('all_frames_record')\n",
    "violation_frames_record_path=create_directory('all_violation_record')\n",
    "\n",
    "while True:    \n",
    "    ret, im = vid.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    im=cv2.resize(im,(1020, 500))\n",
    "    \n",
    "    \n",
    "    results=model.predict(im,conf=0.4,verbose=False,device=[1],classes=[2,3])\n",
    "    # print(f\"\\n----------------------------------------------------------------------------------\\nResult without detection \\n{results[0].boxes.data}\\n----------------------------------------------------------------------------------\\n\")\n",
    "    im=line_plotter(frame=im,x_start=300,x_end=774,y=198,line_color=red_color,line_name='boundry 1',line_thickness=2,text_color=text_color)\n",
    "    im=line_plotter(frame=im,x_start=300,x_end=800,y=280,line_color=blue_color,line_name='boundry 2',line_thickness=2,text_color=text_color)\n",
    "    \n",
    "    # Specify the color of the line in BGR format (here, it's white)\n",
    "    color = (255, 255, 255)\n",
    "    \n",
    "    # Draw the vertical line on the image\n",
    "    thickness = 2  # You can adjust the thickness as needed\n",
    "    cv2.line(im, start_point, end_point, color, thickness)\n",
    "    \n",
    "    # im=cv2.rectangle(im, (300, 198), (800, 280), (255, 255, 255), 2)\n",
    "    \n",
    "    px,conf=prediction_coordinated_hadler(results)\n",
    "    dets = []\n",
    "    # Experimenting\n",
    "    dets=tracker_element_creator(dets,px,conf)        \n",
    "    \n",
    "    # print('out_from')\n",
    "    \n",
    "    dets = np.array(dets)\n",
    "    # print(dets)\n",
    "    if len(dets) > 0:\n",
    "        tracks = tracker.update(dets, im) # --> M X (x, y, x, y, id, conf, cls, ind)\n",
    "        # print('tracks',tracks)    \n",
    "        for track in tracks:\n",
    "                im,object_id,cx,cy,bbox_list=plot_tracks(track,im)\n",
    "                # print(\"vblkjsenjlks\",bbox_list)\n",
    "                all_id_tracker.append(object_id)\n",
    "                # Adding file path for csv\n",
    "                csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'all_frame_detection_detail.csv')\n",
    "            \n",
    "                if red_line_y<(cy+offset) and red_line_y > (cy-offset):\n",
    "                    # print(\"entered if 1\")\n",
    "                    down[object_id]=time.time()\n",
    "            \n",
    "                if object_id in down:\n",
    "                    # print(\"entered if 2\")\n",
    "               \n",
    "                    if blue_line_y<(cy+offset) and blue_line_y > (cy-offset):\n",
    "                         # print(\"entered if 3\")\n",
    "                         elapsed_time=time.time() - down[object_id] \n",
    "                         # print(\"entered if 4\")\n",
    "                         if counter_down.count(object_id)==0:\n",
    "                            counter_down.append(object_id) \n",
    "                            distance = 9\n",
    "                            est_speed=speed_calculator_kmph(distance,elapsed_time)\n",
    "                            if est_speed>20:\n",
    "                                print(f'violation_detected : {object_id}')\n",
    "                                voilation_id_tracker.append(object_id)\n",
    "                                violation_frame_tracker[object_id]=frame_count\n",
    "                                # violation_frame_writer_op=frame_writer(frame_count,im,violation_frames_record_path)\n",
    "                                print(\"bbox_list-\",bbox_list)\n",
    "                                violation_frame_writer_op=frame_writer(frame_count,im,violation_frames_record_path)\n",
    "                                violation_frame_writer_op=frame_writer(frame_count,im,violation_frames_record_path,coordiante_blur=True,bbox_list=bbox_list)\n",
    "                                violation_csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'violation_frame_detection_detail.csv')\n",
    "                                \n",
    "                            print(f\"for {object_id} speed is -> {est_speed}\")\n",
    "    \n",
    "    all_frame_writer_op=frame_writer(frame_count,im,all_frames_record_path)    \n",
    "    frame_count+=1\n",
    "    out_main.write(im)\n",
    "    \n",
    "    # # break on pressing q or spaceq\n",
    "    cv2.imshow('BoxMOT detection', im)     \n",
    "    # key = cv2.waitKey(25) & 0xFF\n",
    "    \n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "remove_model_from_gpu(model)\n",
    "end_time=time.time()\n",
    "print(f'total_time {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d868475e-2f67-4d65-be36-2b3dcae040cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voilation_capture_json_creator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'voilation_capture_json_creator' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_violation_json=voilation_capture_json_creator(voilation_id_tracker,csv_file_path)\n",
    "maintained=json_frame_order_checker(main_violation_json)\n",
    "if maintained:\n",
    "    evidance_img_dir_paths,evidance_clip_dir_paths=evidance_directories_creator(main_violation_json)\n",
    "    evidance_img_dir_paths=evidance_img_separator(evidance_img_dir_paths,all_frames_record_path,main_violation_json)\n",
    "    video_clip_creator_mt(voilation_id_tracker,violation_frame_tracker,evidance_clip_dir_paths,all_frames_record_path)\n",
    "    cropped_images_path_list=evidance_cropper_mt(evidance_img_dir_paths,main_violation_json)\n",
    "    deblurred_image_paths=deblur_images(cropped_images_path_list,main_violation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b5554e-bfbb-481a-9313-f4ee18d13fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 4.06 s, total: 7.61 s\n",
      "Wall time: 54min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using default 8 threads to achive max effiencicy\n",
    "enhance_image_path=image_enhancement_using_limit_mpx(deblurred_image_paths,num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fa204-6e83-4260-bce8-46a7893760a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preprocssing_part_one(enhance_image_path,'models/LPD.pt',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480020ad-99e4-41c5-b329-0f9fa250042f",
   "metadata": {},
   "source": [
    "##### ANRP Phase ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4db3046f-385b-44e8-af75-6ebb66e5a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1m/home/hlink/workspace/anrp/runs/detect/predict5\u001b[0m\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'liscense_plate'}\n",
      "obb: None\n",
      "orig_img: array([[[42, 38, 33],\n",
      "        [42, 38, 33],\n",
      "        [48, 45, 41],\n",
      "        ...,\n",
      "        [23, 27, 28],\n",
      "        [26, 30, 31],\n",
      "        [36, 40, 41]],\n",
      "\n",
      "       [[45, 41, 36],\n",
      "        [43, 39, 34],\n",
      "        [50, 47, 43],\n",
      "        ...,\n",
      "        [19, 23, 24],\n",
      "        [25, 29, 30],\n",
      "        [33, 37, 38]],\n",
      "\n",
      "       [[53, 49, 44],\n",
      "        [49, 45, 40],\n",
      "        [58, 54, 49],\n",
      "        ...,\n",
      "        [26, 30, 31],\n",
      "        [25, 29, 30],\n",
      "        [23, 27, 28]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[41, 46, 45],\n",
      "        [43, 48, 47],\n",
      "        [42, 47, 46],\n",
      "        ...,\n",
      "        [38, 43, 42],\n",
      "        [35, 40, 39],\n",
      "        [33, 38, 37]],\n",
      "\n",
      "       [[15, 17, 17],\n",
      "        [15, 20, 19],\n",
      "        [15, 17, 17],\n",
      "        ...,\n",
      "        [12, 17, 16],\n",
      "        [12, 14, 14],\n",
      "        [ 8, 13, 12]],\n",
      "\n",
      "       [[ 1,  1,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0]]], dtype=uint8)\n",
      "orig_shape: (305, 400)\n",
      "path: '/home/hlink/workspace/learning/boxmot/s.jpg'\n",
      "probs: None\n",
      "save_dir: '/home/hlink/workspace/anrp/runs/detect/predict5'\n",
      "speed: {'preprocess': 24.707317352294922, 'inference': 91.13454818725586, 'postprocess': 13.315439224243164}]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO \n",
    "modelLPD=YOLO('models/LPD.pt')\n",
    "\n",
    "detections = modelLPD.predict('s.jpg',save=True,verbose=False,conf=0.40,device=[1])\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55fac4-452a-49b7-81ca-aae3044a9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import os\n",
    "\n",
    "def anrp(image_path,yolo_model=None,img_enhancer_model_path=None,tr_model=None,tr_device=None,tr_processor=None):\n",
    "    # taking model\n",
    "    model=yolo_model\n",
    "    \n",
    "    # image_name=os.path.split(image_path)\n",
    "    # image_count=os.path.splitext(image_name[0])\n",
    "    \n",
    "    # Reading images\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    detections = model(img,save=False,verbose=False,conf=0.20)\n",
    "    \n",
    "    \n",
    "    x1, y1, x2, y2,detections_=finding_coordinates(detections)\n",
    "\n",
    "    if x1!=-1 and y1!=-1 and x2!=-1 and y2!=-1 and detections!=-1:\n",
    "            x1,y1,w,h=coordinates_creator(x1,y1,x2,y2)\n",
    "            # Cropping number Plate image\n",
    "            cropped_img_number_plate=image_cropper_using_arr(img,x1,y1,w,h)\n",
    "        \n",
    "            upscaled_image=enhance_image(cropped_img_number_plate,re_upscale=True,model_path=img_enhancer_model_path, save_output=True)    \n",
    "            \n",
    "            # ocr_image=image_thereshold_creator(img=cropped_img_number_plate)\n",
    "            \n",
    "            lp_1,lp_2,lp_3=run_ocr(upscaled_image,tr_model,tr_device,tr_processor)\n",
    "            \n",
    "            final_string=common_chars(lp_1,lp_2,lp_3)\n",
    "            \n",
    "            return final_string,cropped_img_number_plate,upscaled_image,ocr_image,image_name[1],image_count\n",
    "    else :\n",
    "        del img\n",
    "        return None,None,None,None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a033e4-4e31-4e01-9ad0-d27ce23de13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_chars(input_1, input_2, input_3):\n",
    "    final_string = ''\n",
    "    for i in range(min(len(input_1), len(input_2), len(input_3))):\n",
    "        char_count = {input_1[i]: 1, input_2[i]: 1, input_3[i]: 1}\n",
    "        if len(char_count) == 1:\n",
    "            final_string += input_1[i]  # Append any character since all are the same\n",
    "        elif len(char_count) == 2 and input_2[i] == input_3[i]:\n",
    "            final_string += input_2[i]  # Append character present in both inputs 2 and 3\n",
    "        else:\n",
    "            sorted_chars = sorted(char_count.keys(), key=lambda x: -char_count[x])\n",
    "            final_string += sorted_chars[0]  # Append the character that appears in more inputs\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53ba24-da73-483d-b62f-d9a2a9a303c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anrp_processor(anrp_folder_paths,yolo_model,img_enhancer_model_path,tr_device,tr_model,tr_processor):\n",
    "    anrp_detections=[]\n",
    "    for i in anrp_folder_paths:\n",
    "        sorted_files=sort_files_by_creation_time(i['main_folder_path'])\n",
    "        final_json={}\n",
    "        np_counter={}\n",
    "        for j in sorted_files:\n",
    "            final_string,cropped_img_number_plate,upscaled_image,ocr_image,image_name,image_count = anrp(image_path=j,yolo_model=yolo_model,img_enhancer_model_path=img_enhancer_model_path,tr_device=tr_device,tr_model=tr_model,tr_processor=tr_processor)\n",
    "            if final_string != None:\n",
    "                    if final_string in np_counter.keys():\n",
    "                        np_counter[final_string]+=1\n",
    "                    else:\n",
    "                        np_counter[final_string]=0\n",
    "        if len(np_counter)!=0:    \n",
    "            liscense_plate,detection_number=find_max_key(np_counter) \n",
    "            confidence=(detection_number/len(sorted_files))*100\n",
    "            final_json['count_id']=i['count_id']\n",
    "            final_json['number_plate_ocr_value']=liscense_plate\n",
    "            final_json['confidence']=confidence\n",
    "            final_json['evidance_number_plate_image']=frame_writer(count=image_count,frame=cropped_img_number_plate,folder_path=i['cropped_folder_path'],resize=False)\n",
    "            final_json['ocr_evidance_number_plate_image']=frame_writer(count=image_count,frame=ocr_image,folder_path=i['ocr_folder_path'],resize=False)\n",
    "            final_json['upscaled_evidance_number_plate_image']=frame_writer(count=image_count,frame=upscaled_image,folder_path=i['upscaled_folder_path'],resize=False)\n",
    "                        \n",
    "            anrp_detections.append(final_json)            \n",
    "        \n",
    "    return anrp_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db18b6-19f2-4486-b3e2-a2b888538b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create an empty tensor with shape (0, 6) on the CUDA device (GPU 0)\n",
    "tensor = torch.empty((0, 6), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b47e3e-14dc-4f04-b501-37f43205384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tensor)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f4b44-fbcc-415d-b2ad-2516c872ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecdd85-c13c-4932-acd2-ae3f980f9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from utils.utils import *\n",
    "\n",
    "def run_tracker_in_thread(filename, model, file_count):\n",
    "    \"\"\"\n",
    "    This function is designed to run a video file or webcam stream\n",
    "    concurrently with the YOLOv8 model, utilizing threading.\n",
    "\n",
    "    - filename: The path to the video file or the webcam/external\n",
    "    camera source.\n",
    "    - model: The file path to the YOLOv8 model.\n",
    "    - file_count: An optional argument to specify the count of the\n",
    "    file being processed.\n",
    "    \"\"\"\n",
    "    dir_path=create_directory(f'mp_test_{file_count}')\n",
    "    \n",
    "    video = cv2.VideoCapture(filename)  # Read the video file\n",
    "    print(f\"Starting for file {file_count}\")\n",
    "    count=0\n",
    "    while True:\n",
    "        ret, frame = video.read()  # Read the video frames\n",
    "        \n",
    "        # Exit the loop if no more frames in either video\n",
    "        if not ret:\n",
    "            break\n",
    "        # Track objects in frames if available\n",
    "        results = model.predict(frame,device=[1],verbose=False)\n",
    "        # res_plotted = results[0].plot()\n",
    "        # cv2.imshow(\"Tracking_Stream_\"+str(file_count), frame)\n",
    "        file_path=os.path.join(dir_path,f'{count}.jpg')\n",
    "        # print(file_path)\n",
    "        cv2.imwrite(file_path,frame)\n",
    "        count=count+1\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    print(f\"finished for {file_count}\")\n",
    "    # Release video sources\n",
    "    video.release()\n",
    "\n",
    "\n",
    "# Define the video files for tracking\n",
    "video_file1 = '/media/hlink/hd/test_videos/testx_vid_1.mp4'\n",
    "video_file2 = '/media/hlink/hd/test_videos/testx_vid_2.mp4'\n",
    "video_file3 = '/media/hlink/hd/test_videos/testx_vid_3.mp4'\n",
    "video_file4 = '/media/hlink/hd/test_videos/testx_vid_4.mp4'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create and start tracking threads\n",
    "tracker_thread1 = threading.Thread(target=run_tracker_in_thread, args=(video_file1, model1, 1), daemon=True)\n",
    "tracker_thread2 = threading.Thread(target=run_tracker_in_thread, args=(video_file2, model2, 2), daemon=True)\n",
    "tracker_thread3 = threading.Thread(target=run_tracker_in_thread, args=(video_file3, model3, 3), daemon=True)\n",
    "tracker_thread4 = threading.Thread(target=run_tracker_in_thread, args=(video_file4, model4, 4), daemon=True)\n",
    "tracker_thread1.start()\n",
    "tracker_thread2.start()\n",
    "tracker_thread3.start()\n",
    "tracker_thread4.start()\n",
    "\n",
    "# Wait for threads to finish\n",
    "tracker_thread1.join()\n",
    "tracker_thread2.join()\n",
    "tracker_thread3.join()\n",
    "tracker_thread4.join()\n",
    "\n",
    "# Clean up and close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad0062f-a56c-410e-b5d6-3ee7b213d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function resize:\n",
      "\n",
      "resize(...)\n",
      "    resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -> dst\n",
      "    .   @brief Resizes an image.\n",
      "    .   \n",
      "    .   The function resize resizes the image src down to or up to the specified size. Note that the\n",
      "    .   initial dst type or size are not taken into account. Instead, the size and type are derived from\n",
      "    .   the `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst,\n",
      "    .   you may call the function as follows:\n",
      "    .   @code\n",
      "    .       // explicitly specify dsize=dst.size(); fx and fy will be computed from that.\n",
      "    .       resize(src, dst, dst.size(), 0, 0, interpolation);\n",
      "    .   @endcode\n",
      "    .   If you want to decimate the image by factor of 2 in each direction, you can call the function this\n",
      "    .   way:\n",
      "    .   @code\n",
      "    .       // specify fx and fy and let the function compute the destination image size.\n",
      "    .       resize(src, dst, Size(), 0.5, 0.5, interpolation);\n",
      "    .   @endcode\n",
      "    .   To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to\n",
      "    .   enlarge an image, it will generally look best with #INTER_CUBIC (slow) or #INTER_LINEAR\n",
      "    .   (faster but still looks OK).\n",
      "    .   \n",
      "    .   @param src input image.\n",
      "    .   @param dst output image; it has the size dsize (when it is non-zero) or the size computed from\n",
      "    .   src.size(), fx, and fy; the type of dst is the same as of src.\n",
      "    .   @param dsize output image size; if it equals zero (`None` in Python), it is computed as:\n",
      "    .    \\f[\\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\\f]\n",
      "    .    Either dsize or both fx and fy must be non-zero.\n",
      "    .   @param fx scale factor along the horizontal axis; when it equals 0, it is computed as\n",
      "    .   \\f[\\texttt{(double)dsize.width/src.cols}\\f]\n",
      "    .   @param fy scale factor along the vertical axis; when it equals 0, it is computed as\n",
      "    .   \\f[\\texttt{(double)dsize.height/src.rows}\\f]\n",
      "    .   @param interpolation interpolation method, see #InterpolationFlags\n",
      "    .   \n",
      "    .   @sa  warpAffine, warpPerspective, remap\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "help(cv2.resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed138a5b-85c8-40dc-8c1c-6e1f8e2a7655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "bmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
