{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cc8703-72b6-4648-9fc7-5ca0c3a9c605",
   "metadata": {},
   "source": [
    "#### PIPE - 7:    \n",
    "This pipe covers the following : \n",
    "1. All features of pipe 6\n",
    "2. Video frame blurring for trackers\n",
    "3. Integration for front end area selection while maintaining the same distance\n",
    "5. Integration of MP queue in phase one\n",
    "6. ANRP Processing phase 2 - Enhance cropped images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2965e-13dd-4de9-847e-7a13e400c061",
   "metadata": {},
   "source": [
    "#### Processing mask cooredinates\n",
    "cv2.rectangle(image, (self.camera.boundary.get('x'), self.camera.boundary.get('y')),(self.camera.boundary.get('x2'), self.camera.boundary.get('y2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc2d88-b0c0-41c1-8bd1-25231dd07540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f7adb2-aa81-4ceb-890e-3f213714c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-22 10:50:10.299\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"models/osnet_x0_25_msmt17.pt\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_detected : 3\n",
      "bbox_list- [484, 237, 581, 319]\n",
      "for 3 speed is -> 42\n",
      "violation_detected : 8\n",
      "bbox_list- [463, 223, 569, 329]\n",
      "for 8 speed is -> 49\n",
      "violation_detected : 25\n",
      "bbox_list- [546, 252, 573, 300]\n",
      "for 25 speed is -> 37\n",
      "violation_detected : 35\n",
      "bbox_list- [611, 255, 641, 298]\n",
      "for 35 speed is -> 32\n",
      "violation_detected : 50\n",
      "bbox_list- [538, 253, 567, 297]\n",
      "for 50 speed is -> 34\n",
      "violation_detected : 67\n",
      "bbox_list- [369, 230, 465, 327]\n",
      "for 67 speed is -> 40\n",
      "violation_detected : 91\n",
      "bbox_list- [531, 249, 563, 302]\n",
      "for 91 speed is -> 32\n",
      "violation_detected : 96\n",
      "bbox_list- [356, 222, 452, 329]\n",
      "for 96 speed is -> 43\n",
      "violation_detected : 100\n",
      "bbox_list- [418, 221, 523, 336]\n",
      "for 100 speed is -> 46\n",
      "violation_detected : 107\n",
      "bbox_list- [613, 255, 647, 298]\n",
      "for 107 speed is -> 29\n",
      "violation_detected : 144\n",
      "bbox_list- [346, 238, 435, 322]\n",
      "for 144 speed is -> 53\n",
      "violation_detected : 151\n",
      "bbox_list- [616, 257, 641, 297]\n",
      "for 151 speed is -> 28\n",
      "violation_detected : 138\n",
      "bbox_list- [464, 236, 554, 314]\n",
      "for 138 speed is -> 35\n",
      "violation_detected : 155\n",
      "bbox_list- [462, 237, 545, 316]\n",
      "for 155 speed is -> 34\n",
      "violation_detected : 164\n",
      "bbox_list- [417, 228, 516, 323]\n",
      "for 164 speed is -> 46\n",
      "violation_detected : 167\n",
      "bbox_list- [437, 237, 535, 322]\n",
      "for 167 speed is -> 55\n",
      "violation_detected : 176\n",
      "bbox_list- [436, 229, 541, 328]\n",
      "for 176 speed is -> 34\n",
      "for 179 speed is -> 13\n",
      "violation_detected : 185\n",
      "bbox_list- [333, 239, 421, 323]\n",
      "for 185 speed is -> 64\n",
      "violation_detected : 187\n",
      "bbox_list- [357, 237, 445, 318]\n",
      "for 187 speed is -> 56\n",
      "violation_detected : 195\n",
      "bbox_list- [402, 227, 513, 339]\n",
      "for 195 speed is -> 49\n",
      "violation_detected : 202\n",
      "bbox_list- [527, 232, 621, 322]\n",
      "for 202 speed is -> 38\n",
      "violation_detected : 208\n",
      "bbox_list- [500, 239, 600, 320]\n",
      "for 208 speed is -> 53\n",
      "violation_detected : 207\n",
      "bbox_list- [394, 230, 493, 324]\n",
      "for 207 speed is -> 40\n",
      "violation_detected : 238\n",
      "bbox_list- [385, 234, 484, 327]\n",
      "for 238 speed is -> 54\n",
      "violation_detected : 245\n",
      "bbox_list- [494, 225, 608, 329]\n",
      "for 245 speed is -> 37\n",
      "violation_detected : 268\n",
      "bbox_list- [491, 238, 593, 321]\n",
      "for 268 speed is -> 30\n",
      "violation_detected : 279\n",
      "bbox_list- [454, 256, 489, 301]\n",
      "for 279 speed is -> 47\n",
      "violation_detected : 290\n",
      "bbox_list- [344, 225, 436, 329]\n",
      "for 290 speed is -> 45\n",
      "violation_detected : 296\n",
      "bbox_list- [482, 236, 576, 319]\n",
      "for 296 speed is -> 31\n",
      "violation_detected : 310\n",
      "bbox_list- [620, 257, 648, 297]\n",
      "for 310 speed is -> 33\n",
      "violation_detected : 315\n",
      "bbox_list- [566, 255, 596, 301]\n",
      "for 315 speed is -> 36\n",
      "violation_detected : 357\n",
      "bbox_list- [623, 258, 649, 298]\n",
      "for 357 speed is -> 38\n",
      "violation_detected : 370\n",
      "bbox_list- [531, 254, 570, 301]\n",
      "for 370 speed is -> 36\n",
      "violation_detected : 369\n",
      "bbox_list- [391, 224, 487, 328]\n",
      "for 369 speed is -> 53\n",
      "violation_detected : 378\n",
      "bbox_list- [474, 232, 580, 326]\n",
      "for 378 speed is -> 42\n",
      "violation_detected : 394\n",
      "bbox_list- [572, 256, 598, 300]\n",
      "for 394 speed is -> 41\n",
      "violation_detected : 466\n",
      "bbox_list- [563, 253, 595, 298]\n",
      "for 466 speed is -> 27\n",
      "violation_detected : 544\n",
      "bbox_list- [449, 224, 555, 336]\n",
      "for 544 speed is -> 49\n",
      "violation_detected : 603\n",
      "bbox_list- [640, 254, 670, 297]\n",
      "for 603 speed is -> 26\n",
      "violation_detected : 598\n",
      "bbox_list- [467, 224, 578, 327]\n",
      "for 598 speed is -> 31\n",
      "total_time 853.6660957336426\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import threading \n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from boxmot import StrongSORT\n",
    "from utils.utils import *\n",
    "start_time=time.time()\n",
    "down = {}\n",
    "up = {}\n",
    "text_color = (0, 0, 0)  # Black color for text\n",
    "yellow_color = (0, 255, 255)  # Yellow color for background\n",
    "red_color = (0, 0, 255)  # Red color for lines\n",
    "blue_color = (255, 0, 0)  # Blue color for lines\n",
    "\n",
    "counter_down = []\n",
    "counter_up = []\n",
    "\n",
    "first_boundry_y=170\n",
    "\n",
    "red_line_y = 198\n",
    "\n",
    "blue_line_y = 280\n",
    "\n",
    "offset = 6\n",
    "# Specify the start and end points of the line\n",
    "start_point = (300, 198)\n",
    "end_point = (300, 280)\n",
    "\n",
    "all_id_tracker=[]\n",
    "voilation_id_tracker=[]\n",
    "violation_frame_tracker={}\n",
    "tracker=tracker_init(cuda_device=torch.cuda.is_available(),cuda_device_number=1)\n",
    "model=YOLO('yolov8s.pt')\n",
    "video_path='/media/hlink/hd/test_videos/testx_vid_1.mp4'\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_main = cv2.VideoWriter('output_strongs.avi', fourcc, 20.0, (1020, 500))\n",
    "frame_count=0\n",
    "all_frames_record_path=create_directory('all_frames_record')\n",
    "violation_frames_record_path=create_directory('all_violation_record')\n",
    "x_start=300\n",
    "x_end=800\n",
    "y_start=198\n",
    "y_end=280\n",
    "\n",
    "while True:    \n",
    "    ret, frame = vid.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    resized_image=cv2.resize(frame,(1020, 500))\n",
    "    blurred_image=blur_except_rectangle(resized_image,x_start,y_start,x_end,y_end,offset_size=[30,80],blur_kernel_size=(15,15))\n",
    "    \n",
    "    results=model.predict(blurred_image,conf=0.4,verbose=False,device=[1],classes=[2,3])\n",
    "    # print(f\"\\n----------------------------------------------------------------------------------\\nResult without detection \\n{results[0].boxes.data}\\n----------------------------------------------------------------------------------\\n\")\n",
    "    blurred_image=line_plotter(frame=blurred_image,x_start=300,x_end=774,y=198,line_color=red_color,line_name='boundry 1',line_thickness=2,text_color=text_color)\n",
    "    blurred_image=line_plotter(frame=blurred_image,x_start=300,x_end=800,y=280,line_color=blue_color,line_name='boundry 2',line_thickness=2,text_color=text_color)\n",
    "    \n",
    "    # Specify the color of the line in BGR format (here, it's white)\n",
    "    color = (255, 255, 255)\n",
    "    \n",
    "    # Draw the vertical line on the image\n",
    "    thickness = 2  # You can adjust the thickness as needed\n",
    "    cv2.line(blurred_image, start_point, end_point, color, thickness)\n",
    "    \n",
    "    # im=cv2.rectangle(blurred_image, (300, 198), (800, 280), (255, 255, 255), 2)\n",
    "    \n",
    "    px,conf=prediction_coordinated_hadler(results)\n",
    "    dets = []\n",
    "    # Experimenting\n",
    "    dets=tracker_element_creator(dets,px,conf)        \n",
    "    \n",
    "    # print('out_from')\n",
    "    \n",
    "    dets = np.array(dets)\n",
    "    # print(dets)\n",
    "    if len(dets) > 0:\n",
    "        tracks = tracker.update(dets, blurred_image) # --> M X (x, y, x, y, id, conf, cls, ind)\n",
    "        # print('tracks',tracks)    \n",
    "        for track in tracks:\n",
    "                blurred_image,object_id,cx,cy,bbox_list=plot_tracks(track,blurred_image)\n",
    "                \n",
    "                # print(\"vblkjsenjlks\",bbox_list)\n",
    "                all_id_tracker.append(object_id)\n",
    "                \n",
    "                # Adding file path for csv\n",
    "                csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'all_frame_detection_detail.csv')\n",
    "            \n",
    "                if red_line_y<(cy+offset) and red_line_y > (cy-offset):\n",
    "                    # print(\"entered if 1\")\n",
    "                    down[object_id]=time.time()\n",
    "            \n",
    "                if object_id in down:\n",
    "                    # print(\"entered if 2\")\n",
    "               \n",
    "                    if blue_line_y<(cy+offset) and blue_line_y > (cy-offset):\n",
    "                         # print(\"entered if 3\")\n",
    "                         elapsed_time=time.time() - down[object_id] \n",
    "                         # print(\"entered if 4\")\n",
    "                         if counter_down.count(object_id)==0:\n",
    "                            counter_down.append(object_id) \n",
    "                            distance = 9\n",
    "                            est_speed=speed_calculator_kmph(distance,elapsed_time)\n",
    "                            if est_speed>20:\n",
    "                                print(f'violation_detected : {object_id}')\n",
    "                                voilation_id_tracker.append(object_id)\n",
    "                                violation_frame_tracker[object_id]=frame_count\n",
    "                                # violation_frame_writer_op=frame_writer(frame_count,im,violation_frames_record_path)\n",
    "                                print(\"bbox_list-\",bbox_list)\n",
    "                                violation_frame_writer_op=frame_writer(frame_count,resized_image,violation_frames_record_path)\n",
    "                                violation_frame_writer_op=frame_writer(frame_count,resized_image,violation_frames_record_path,coordiante_blur=True,bbox_list=bbox_list)\n",
    "                                violation_csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'violation_frame_detection_detail.csv')\n",
    "                                \n",
    "                            print(f\"for {object_id} speed is -> {est_speed}\")\n",
    "    \n",
    "    all_frame_writer_op=frame_writer(frame_count,resized_image,all_frames_record_path)    \n",
    "    frame_count+=1\n",
    "    out_main.write(resized_image)\n",
    "    \n",
    "    # # break on pressing q or spaceq\n",
    "    cv2.imshow('Strong Sort Detection detection', blurred_image)     \n",
    "    # key = cv2.waitKey(25) & 0xFF\n",
    "    \n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "remove_model_from_gpu(model)\n",
    "end_time=time.time()\n",
    "print(f'total_time {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92ba0ba-e776-404f-a5e5-67a9a22fe443",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voilation_capture_json_creator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'voilation_capture_json_creator' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_violation_json=voilation_capture_json_creator(voilation_id_tracker,csv_file_path)\n",
    "maintained=json_frame_order_checker(main_violation_json)\n",
    "if maintained:\n",
    "    evidance_img_dir_paths,evidance_clip_dir_paths=evidance_directories_creator(main_violation_json)\n",
    "    evidance_img_dir_paths=evidance_img_separator(evidance_img_dir_paths,all_frames_record_path,main_violation_json)\n",
    "    video_clip_creator_mt(voilation_id_tracker,violation_frame_tracker,evidance_clip_dir_paths,all_frames_record_path)\n",
    "    cropped_images_path_list=evidance_cropper_mt(evidance_img_dir_paths,main_violation_json)\n",
    "    deblurred_image_paths=deblur_images(cropped_images_path_list,main_violation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099f956-bebf-4f39-889c-836635368f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using default 8 threads to achive max effiencicy\n",
    "enhance_image_path=image_enhancement_using_limit_mpx(deblurred_image_paths,num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01791da4-b3ff-4d94-a1f6-5ce44ee14bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-103 (sub_process_for_preprocess_one):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hlink/workspace/learning/boxmot/utils/utils.py\", line 778, in sub_process_for_preprocess_one\n",
      "    idx=i.split('/')[-1]\n",
      "                     ^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 453, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 168, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 228, in stream_inference\n",
      "    self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 627, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 453, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 89, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 107, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 128, in _predict_once\n",
      "    x = m(x)  # run\n",
      "        ^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py\", line 54, in forward_fuse\n",
      "    return self.act(self.conv(x))\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "Exception in thread Thread-102 (sub_process_for_preprocess_one):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hlink/workspace/learning/boxmot/utils/utils.py\", line 778, in sub_process_for_preprocess_one\n",
      "    idx=i.split('/')[-1]\n",
      "                     ^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 453, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 168, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 228, in stream_inference\n",
      "    self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 627, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 453, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 89, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 107, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 128, in _predict_once\n",
      "    x = m(x)  # run\n",
      "        ^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py\", line 54, in forward_fuse\n",
      "    return self.act(self.conv(x))\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 918 ms, sys: 982 ms, total: 1.9 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cropped_lp_paths=preprocssing_part_one(enhance_image_path,'models/LPD.pt',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314e68ec-f3a1-4741-93f8-27fd536fb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_cropped_images_lp=image_enhancement_using_limit_mpx(cropped_lp_paths,num_threads=8,dir_name='cropped_lp_enhanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c4f17-b8f5-4ba4-927a-64aecf8e806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_chars(input_1, input_2, input_3):\n",
    "    final_string = ''\n",
    "    for i in range(min(len(input_1), len(input_2), len(input_3))):\n",
    "        char_count = {input_1[i]: 1, input_2[i]: 1, input_3[i]: 1}\n",
    "        if len(char_count) == 1:\n",
    "            final_string += input_1[i]  # Append any character since all are the same\n",
    "        elif len(char_count) == 2 and input_2[i] == input_3[i]:\n",
    "            final_string += input_2[i]  # Append character present in both inputs 2 and 3\n",
    "        else:\n",
    "            sorted_chars = sorted(char_count.keys(), key=lambda x: -char_count[x])\n",
    "            final_string += sorted_chars[0]  # Append the character that appears in more inputs\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d618a57-e508-4e17-b04a-3b9333a79741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anrp_processor(anrp_folder_paths,yolo_model,img_enhancer_model_path,tr_device,tr_model,tr_processor):\n",
    "    anrp_detections=[]\n",
    "    for i in anrp_folder_paths:\n",
    "        sorted_files=sort_files_by_creation_time(i['main_folder_path'])\n",
    "        final_json={}\n",
    "        np_counter={}\n",
    "        for j in sorted_files:\n",
    "            final_string,cropped_img_number_plate,upscaled_image,ocr_image,image_name,image_count = anrp(image_path=j,yolo_model=yolo_model,img_enhancer_model_path=img_enhancer_model_path,tr_device=tr_device,tr_model=tr_model,tr_processor=tr_processor)\n",
    "            if final_string != None:\n",
    "                    if final_string in np_counter.keys():\n",
    "                        np_counter[final_string]+=1\n",
    "                    else:\n",
    "                        np_counter[final_string]=0\n",
    "        if len(np_counter)!=0:    \n",
    "            liscense_plate,detection_number=find_max_key(np_counter) \n",
    "            confidence=(detection_number/len(sorted_files))*100\n",
    "            final_json['count_id']=i['count_id']\n",
    "            final_json['number_plate_ocr_value']=liscense_plate\n",
    "            final_json['confidence']=confidence\n",
    "            final_json['evidance_number_plate_image']=frame_writer(count=image_count,frame=cropped_img_number_plate,folder_path=i['cropped_folder_path'],resize=False)\n",
    "            final_json['ocr_evidance_number_plate_image']=frame_writer(count=image_count,frame=ocr_image,folder_path=i['ocr_folder_path'],resize=False)\n",
    "            final_json['upscaled_evidance_number_plate_image']=frame_writer(count=image_count,frame=upscaled_image,folder_path=i['upscaled_folder_path'],resize=False)\n",
    "                        \n",
    "            anrp_detections.append(final_json)            \n",
    "        \n",
    "    return anrp_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a9f0e-461e-46c5-9e76-c69baebbd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_1,lp_2,lp_3=run_ocr(upscaled_image,tr_model,tr_device,tr_processor)\n",
    "            \n",
    "final_string=common_chars(lp_1,lp_2,lp_3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894ac6c-2375-4b36-8c50-fb1f5200b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8854c-c63b-404f-b47a-95b5d41f0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "OCR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "bmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
