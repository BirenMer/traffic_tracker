{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df474b98-c345-42d4-a8e6-80ea1c2ed614",
   "metadata": {},
   "source": [
    "#### PIPE - 5:    \n",
    "This pipe covers the following : \n",
    "1. All features of pipe 4\n",
    "2. Multi Thread implementation for image upscaling function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea62545-b625-41a9-8f34-0f6bbe7ec740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190464b8-c45e-4f1e-9c6c-1012a4e6dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create function to load models and trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92249fc-148c-4ae0-91eb-d415356c0e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-15 11:55:37.739\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"models/osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "/home/hlink/workspace/learning/boxmot/bmt/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time 10.350139141082764\n",
      "CPU times: user 11.2 s, sys: 14 s, total: 25.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import threading \n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from boxmot import StrongSORT\n",
    "from utils.utils import *\n",
    "start_time=time.time()\n",
    "down = {}\n",
    "up = {}\n",
    "text_color = (0, 0, 0)  # Black color for text\n",
    "yellow_color = (0, 255, 255)  # Yellow color for background\n",
    "red_color = (0, 0, 255)  # Red color for lines\n",
    "blue_color = (255, 0, 0)  # Blue color for lines\n",
    "\n",
    "counter_down = []\n",
    "counter_up = []\n",
    "\n",
    "first_boundry_y=170\n",
    "\n",
    "red_line_y = 198\n",
    "\n",
    "blue_line_y = 280\n",
    "\n",
    "offset = 6\n",
    "# Specify the start and end points of the line\n",
    "start_point = (300, 198)\n",
    "end_point = (300, 280)\n",
    "\n",
    "all_id_tracker=[]\n",
    "voilation_id_tracker=[]\n",
    "violation_frame_tracker={}\n",
    "tracker=tracker_init(cuda_device=torch.cuda.is_available(),cuda_device_number=1)\n",
    "model=YOLO('yolov8s.pt')\n",
    "video_path='/media/hlink/hd/test_videos/testx_vid_1.mp4'\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_main = cv2.VideoWriter('output_strongs.avi', fourcc, 20.0, (1020, 500))\n",
    "frame_count=0\n",
    "all_frames_record_path=create_directory('all_frames_record')\n",
    "violation_frames_record_path=create_directory('all_violation_record')\n",
    "\n",
    "while True:    \n",
    "    ret, im = vid.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    im=cv2.resize(im,(1020, 500))\n",
    "    \n",
    "    \n",
    "    results=model.predict(im,conf=0.4,verbose=False,device=[1],classes=[2,3])\n",
    "    im=line_plotter(frame=im,x_start=300,x_end=774,y=198,line_color=red_color,line_name='boundry 1',line_thickness=2,text_color=text_color)\n",
    "    im=line_plotter(frame=im,x_start=300,x_end=800,y=280,line_color=blue_color,line_name='boundry 2',line_thickness=2,text_color=text_color)\n",
    "    \n",
    "    # Specify the color of the line in BGR format (here, it's white)\n",
    "    color = (255, 255, 255)\n",
    "    \n",
    "    # Draw the vertical line on the image\n",
    "    thickness = 2  # You can adjust the thickness as needed\n",
    "    cv2.line(im, start_point, end_point, color, thickness)\n",
    "    \n",
    "    # im=cv2.rectangle(im, (300, 198), (800, 280), (255, 255, 255), 2)\n",
    "    \n",
    "    px,conf=prediction_coordinated_hadler(results)\n",
    "    dets = []\n",
    "    # Experimenting\n",
    "    dets=tracker_element_creator(dets,px,conf)        \n",
    "    \n",
    "    # print('out_from')\n",
    "    \n",
    "    dets = np.array(dets)\n",
    "    # print(dets)\n",
    "    if len(dets) > 0:\n",
    "        tracks = tracker.update(dets, im) # --> M X (x, y, x, y, id, conf, cls, ind)\n",
    "        # print('tracks',tracks)    \n",
    "        for track in tracks:\n",
    "                im,object_id,cx,cy,bbox_list=plot_tracks(track,im)\n",
    "               \n",
    "                all_id_tracker.append(object_id)\n",
    "                # Adding file path for csv\n",
    "                csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'all_frame_detection_detail.csv')\n",
    "            \n",
    "                if red_line_y<(cy+offset) and red_line_y > (cy-offset):\n",
    "                    # print(\"entered if 1\")\n",
    "                    down[object_id]=time.time()\n",
    "            \n",
    "                if object_id in down:\n",
    "                    # print(\"entered if 2\")\n",
    "               \n",
    "                    if blue_line_y<(cy+offset) and blue_line_y > (cy-offset):\n",
    "                         # print(\"entered if 3\")\n",
    "                         elapsed_time=time.time() - down[object_id] \n",
    "                         # print(\"entered if 4\")\n",
    "                         if counter_down.count(object_id)==0:\n",
    "                            counter_down.append(object_id) \n",
    "                            distance = 9\n",
    "                            est_speed=speed_calculator_kmph(distance,elapsed_time)\n",
    "                            if est_speed>20:\n",
    "                                print(f'violation_detected : {object_id}')\n",
    "                                voilation_id_tracker.append(object_id)\n",
    "                                violation_frame_tracker[object_id]=frame_count\n",
    "                                violation_frame_writer_op=frame_writer(frame_count,im,violation_frames_record_path)\n",
    "                                violation_csv_file_path=detection_coordinate_write(frame_count,object_id,bbox_list,'violation_frame_detection_detail.csv')\n",
    "                                \n",
    "                            print(f\"for {object_id} speed is -> {est_speed}\")\n",
    "    \n",
    "    all_frame_writer_op=frame_writer(frame_count,im,all_frames_record_path)    \n",
    "    frame_count+=1\n",
    "    out_main.write(im)\n",
    "    \n",
    "    # # break on pressing q or spaceq\n",
    "    cv2.imshow('BoxMOT detection', im)     \n",
    "    # key = cv2.waitKey(25) & 0xFF\n",
    "    \n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "end_time=time.time()\n",
    "print(f'total_time {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b98a52-b162-468f-a20f-7999459d5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "order_maintainer - True\n",
      "CPU times: user 209 ms, sys: 1.76 s, total: 1.97 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_violation_json=voilation_capture_json_creator(voilation_id_tracker,csv_file_path)\n",
    "maintained=json_frame_order_checker(main_violation_json)\n",
    "if maintained:\n",
    "    evidance_img_dir_paths,evidance_clip_dir_paths=evidance_directories_creator(main_violation_json)\n",
    "    evidance_img_dir_paths=evidance_img_separator(evidance_img_dir_paths,all_frames_record_path,main_violation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d564e29-801d-4c20-80fb-4ee10c3621d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 18.6 s, total: 44.5 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if maintained:\n",
    "    video_clip_creator_mt(voilation_id_tracker,violation_frame_tracker,evidance_clip_dir_paths,all_frames_record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f893e959-6243-4a33-ad2e-e8c4fecbaced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.7 s, sys: 1.41 s, total: 45.1 s\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if maintained:\n",
    "    cropped_images_path_list=evidance_cropper_mt(evidance_img_dir_paths,main_violation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336b9ad4-3e28-4018-8568-764258509d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/5',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/10',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/28',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/34',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/72',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/88',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/107',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/113',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/174',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/169',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/180',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/213',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/214',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/220',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/223',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/239',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/241',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/274',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/263',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/315',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/320',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/367',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/401',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/422',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/455',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/522',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/530',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/557',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/637',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/650',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/735',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/742',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/794',\n",
       " '/home/hlink/workspace/learning/boxmot/14_05_2024_infenrence_output/cropped_evidance_images/852']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_images_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "688d32bf-9294-4911-871a-b96d00806ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 ms, sys: 300 ms, total: 871 ms\n",
      "Wall time: 908 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deblurred_image_paths=deblur_images(cropped_images_path_list,main_violation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a6299e-db7b-4834-be93-1d505d1fed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.35 s, sys: 4.68 s, total: 9.02 s\n",
      "Wall time: 1h 38min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using default 8 threads to achive max effiencicy\n",
    "enhance_image_path=image_enhancement_using_limit_mpx(deblurred_image_paths,num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe940b9-e2a8-41a4-a556-9a54a64e9038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60787239-1b20-49eb-9c52-0af5397aac42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8e522-2fa0-4e3e-b756-2078106f52fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fb2f0-dd43-4196-a774-66d02e167bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe745874-d09f-4952-b3ca-e24dc3bbd545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db7bd4-1cc6-4f03-a239-93512931ee64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255312be-589a-463d-917f-e3e4774bc749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a870c-c329-4d8b-b2d0-f64db019c434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00575356-c53e-4f97-9b5b-0d204d1be2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77e003-88fe-41c9-bce6-d6afc3028ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b4880-f18b-43a5-8228-fd35bca79b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2cb8d-ba2a-4686-85e7-74de829a624d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25e159-eb70-41e0-8d2d-4cb429b35866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef975d-3f47-4c00-8eed-642dffe8ec83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7c65e-f15f-42cf-8169-f263d8d0589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_clip_thread(entry,violation_frame_tracker,all_frames_record_path,window_size=20):\n",
    "        previous_frames,post_frames=frame_picker(violation_frame_tracker[entry[0]],all_frames_record_path,window_size)\n",
    "        input_frame_list=previous_frames+post_frames\n",
    "        evidance_video_creator(input_frame_list,f'evidance_{entry[0]}',output_folder_path=entry[2])\n",
    "def video_clip_creator_mt(voilation_id_tracker,violation_frame_tracker,evidance_clip_dir_paths,all_frames_record_path):\n",
    "    \"\"\"\n",
    "    Used to create and store video clips from the give frame paths to the given path\n",
    "    \"\"\"\n",
    "    thread_list_=[]\n",
    "    for i in zip(voilation_id_tracker,violation_frame_tracker,evidance_clip_dir_paths):    \n",
    "        t=threading.Thread(target=video_clip_thread,args[i,violation_frame_tracker,all_frames_record_path])\n",
    "        t.start()\n",
    "        thread_list_.append(t)\n",
    "    for j in thread_list_:\n",
    "        j.join()\n",
    "    return evidance_clip_dir_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849c539-d0ef-4b5e-ac65-e51f7bf0bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_function(thread_number):\n",
    "    start_time=time.time()\n",
    "    print(f\"Straing it : {start_time}\")\n",
    "    time.sleep(5)\n",
    "    end_time=time.time()\n",
    "    print(f\"ending it at :{end_time}\")\n",
    "    total_time=end_time-start_time\n",
    "    print(f\"total_time for thread - {thread_number} is {total_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106337a7-55d6-4eaf-82f0-17f1a534990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    list_of_var=[]\n",
    "    t=threading.Thread(target=timer_function,args=[i])\n",
    "    t.start()\n",
    "    list_of_var.append(t)\n",
    "for j in list_of_var:\n",
    "    j.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be8663-464e-4697-873c-28cb8ddcac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def foo(bar):\n",
    "        print('hello {0}'.format(bar))\n",
    "        return \"foo\"\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs, Verbose)\n",
    "        self._return = None\n",
    "    def run(self):\n",
    "        if self._Thread__target is not None:\n",
    "            self._return = self._Thread__target(*self._Thread__args,\n",
    "                                                **self._Thread__kwargs)\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self._return\n",
    "\n",
    "twrv = ThreadWithReturnValue(target=foo, args=('world!',))\n",
    "\n",
    "twrv.start()\n",
    "print(twrv.join())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340b620-6ad9-456b-bace-d3f6606065cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "bmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
